import os
import logging
import re
import ollama  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ollama
from aiogram import Bot, Dispatcher, types
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton, File
from aiogram.utils import executor
import openai
from typing import Optional, Dict
import json
import psycopg2
from psycopg2 import sql
from pathlib import Path
import whisper
from pydub import AudioSegment


#______________________________________________________________________
#DARTABASE

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
DB_NAME = "englishschoolai"
DB_USER = "dezmoond"
DB_PASSWORD = "3621393258lL"
DB_HOST = "localhost"
DB_PORT = "5432"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü
def setup_database():
    conn = psycopg2.connect(dbname='postgres', user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT)
    conn.autocommit = True
    cur = conn.cursor()

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    cur.execute(f"SELECT 1 FROM pg_database WHERE datname = '{DB_NAME}'")
    if not cur.fetchone():
        cur.execute(sql.SQL("CREATE DATABASE {}").format(sql.Identifier(DB_NAME)))
    cur.close()
    conn.close()

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Ü–µ–ª–µ–≤–æ–π –±–∞–∑–µ
    conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT)
    cur = conn.cursor()

    # –¢–∞–±–ª–∏—Ü—ã
    cur.execute("""
    CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    user_id BIGINT UNIQUE,  -- –¢–µ–ª–µ–≥—Ä–∞–º ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    name VARCHAR(100),
    email VARCHAR(100) UNIQUE,
    start_date DATE DEFAULT CURRENT_DATE,
    interests TEXT,
    gender VARCHAR(10)
);
    """)

    cur.execute("""
    CREATE TABLE IF NOT EXISTS daily_analysis (
    id SERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,  -- –°—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ user_id
    analysis_date DATE DEFAULT CURRENT_DATE,
    cefr_level VARCHAR(10),
    read_level VARCHAR(100),
    recommendations TEXT,
    key_mistakes JSONB,
    total_words INTEGER,
    unique_words INTEGER,
    inversions INTEGER,
    participles INTEGER,
    gerunds INTEGER,
    spelling_errors INTEGER,
    grammar_errors INTEGER,
    syntax_errors INTEGER
);
    """)

    cur.execute("""
    CREATE TABLE IF NOT EXISTS weekly_analysis (
    id SERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,  -- –°—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ user_id
    week_number INTEGER,
    month INTEGER,
    year INTEGER,
    avg_cefr_level VARCHAR(10),
    read_level VARCHAR(100),
    recommendations TEXT,
    key_mistakes JSONB,
    total_words INTEGER,
    unique_words INTEGER,
    inversions INTEGER,
    participles INTEGER,
    gerunds INTEGER,
    spelling_errors INTEGER,
    grammar_errors INTEGER,
    syntax_errors INTEGER
);""")

    cur.execute("""CREATE TABLE IF NOT EXISTS monthly_analysis (
    id SERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,  -- –°—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ user_id
    month INTEGER,
    year INTEGER,
    avg_cefr_level VARCHAR(10),
    read_level VARCHAR(100),
    recommendations TEXT,
    key_mistakes JSONB,
    total_words INTEGER,
    unique_words INTEGER,
    inversions INTEGER,
    participles INTEGER,
    gerunds INTEGER,
    spelling_errors INTEGER,
    grammar_errors INTEGER,
    syntax_errors INTEGER
);""")

    cur.execute("""CREATE TABLE IF NOT EXISTS yearly_analysis (
    id SERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,  -- –°—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ user_id
    year INTEGER,
    avg_cefr_level VARCHAR(10),
    read_level VARCHAR(100),
    recommendations TEXT,
    key_mistakes JSONB,
    total_words INTEGER,
    unique_words INTEGER,
    inversions INTEGER,
    participles INTEGER,
    gerunds INTEGER,
    spelling_errors INTEGER,
    grammar_errors INTEGER,
    syntax_errors INTEGER
);""")
    conn.commit()
    cur.close()
    conn.close()

#_____________________________________________________________________
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_TOKEN = '7935445281:AAFNhbIuOtDGhqZJlitA4T_sU5Ytx0tIIog'
USE_OLLAMA = True  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
OLLAMA_MODEL = "llama3.2-vision:latest"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å –≤ ollama, –Ω–∞–ø—Ä–∏–º–µ—Ä "mistral" –∏–ª–∏ "llama2"

# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è OpenAI
# OPENAI_API_KEY = 'your-openai-key'
# openai.api_key = OPENAI_API_KEY

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

whisper_model = whisper.load_model("base")  # –∏–ª–∏ tiny / small / medium
bot = Bot(token=API_TOKEN)
dp = Dispatcher(bot)
user_messages = {}

# –ö–Ω–æ–ø–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Ä–æ–≤–Ω—è
keyboard = ReplyKeyboardMarkup(resize_keyboard=True)
button = KeyboardButton("–û—Ü–µ–Ω–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å")
keyboard.add(button)

CEFR_LEVELS = {
    "A": "–ù–∞—á–∞–ª—å–Ω—ã–π",
    "B": "–°—Ä–µ–¥–Ω–∏–π",
    "C": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π"
}

#_______________________________________________________________
#voice

#async def voice_analyze_with_ollama(text: str) -> str:
#    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ Ollama"""
#    prompt = f"""
#<s>[INST] –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—é –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç, —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π –∏–∑ —Ä–µ—á–∏:
#
#{text}
#
#1. –û—Ü–µ–Ω–∏:
#   - –ï—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–ª–æ—Ö–æ–≥–æ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è (—É–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞)
#   - –ù–∞–ª–∏—á–∏–µ –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–ª–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
#   - –£—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è (1-10)
#
#2. –î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:
#   - –ö–∞–∫–∏–µ –∑–≤—É–∫–∏ –Ω—É–∂–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å
#   - –ö–∞–∫–∏–µ —Å–ª–æ–≤–∞ –ø—Ä–æ–∏–∑–Ω–æ—Å—è—Ç—Å—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
#   - –°–æ–≤–µ—Ç—ã –ø–æ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏ –∏ —Ä–∏—Ç–º—É
#
#3. –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
#   - –ß–µ—Ç–∫–∏–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
#   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
#   - –û—Ü–µ–Ω–∫–∞ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏ [/INST]
#"""
#    try:
#        response = ollama.chat(model=OLLAMA_MODEL, messages=[{"role": "user", "content": prompt}])
#        return response['message'].get('content', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ')
#    except Exception as e:
#        logger.error(f"Voice analysis error: {str(e)}")
#        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è: {str(e)}"
#

# –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
async def handle_file(file: File, file_name: str, path: str):
    try:
        Path(path).mkdir(parents=True, exist_ok=True)
        destination_path = os.path.join(path, file_name)
        logger.info(f"[handle_file] –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {path}")
        logger.info(f"[handle_file] –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {destination_path}")

        file_stream = await bot.download_file(file.file_path)
        logger.info(f"[handle_file] –§–∞–π–ª –ø–æ–ª—É—á–µ–Ω —Å file_path: {file.file_path}")

        with open(destination_path, "wb") as f:
            f.write(file_stream.read())
        logger.info(f"[handle_file] –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {destination_path}")

        return destination_path
    except Exception as e:
        logger.error(f"[handle_file] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        raise



def convert_ogg_to_wav(ogg_path: str) -> str:
    wav_path = ogg_path.replace(".ogg", ".wav")
    try:
        sound = AudioSegment.from_file(ogg_path, format="ogg")
        sound.export(wav_path, format="wav")
        return wav_path
    except Exception as e:
        logger.error(f"[convert_ogg_to_wav] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
        return None

# –•–µ–Ω–¥–ª–µ—Ä –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message_handler(content_types=types.ContentType.VOICE)
async def handle_voice(message: types.Message):
    ogg_path = None
    try:
        user_id = message.from_user.id
        file_id = message.voice.file_id
        logger.info(f"[handle_voice] ‚ñ∂Ô∏è –ü–æ–ª—É—á–µ–Ω–æ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, file_id: {file_id}")

        file: File = await bot.get_file(file_id)
        logger.info(f"[handle_voice] üì• –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –ø–æ–ª—É—á–µ–Ω–∞: {file.file_path}")

        voice_dir = "voices"
        ogg_name = f"audio_{user_id}_{message.message_id}.ogg"
        logger.info(f"[handle_voice] üìÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞: {ogg_name}")

        ogg_path = await handle_file(file=file, file_name=ogg_name, path=voice_dir)

        if not os.path.exists(ogg_path):
            logger.warning(f"[handle_voice] ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {ogg_path}")
            await message.reply("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
            return

        ogg_path = os.path.abspath(ogg_path).replace("\\", "/")
        logger.info(f"[handle_voice] ‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {ogg_path}")

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ Whisper
        logger.info(f"[handle_voice] üß† –ó–∞–ø—É—Å–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ñ–∞–π–ª–∞: {ogg_path}")
        result = whisper_model.transcribe(ogg_path)
        recognized_text = result.get("text", "").strip()
        logger.info(f"[handle_voice] üìú –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {recognized_text}")

        if not recognized_text:
            logger.warning(f"[handle_voice] ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.")
            await message.reply("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç.")
            return

        user_messages.setdefault(user_id, []).append(recognized_text)
        await message.reply(f"üí¨ –í–∞—à —Ç–µ–∫—Å—Ç:\n\n{recognized_text}")

        ## –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è
        #logger.info(f"[handle_voice] üßë‚Äçüè´ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ Ollama...")
        #pronunciation_feedback = await voice_analyze_with_ollama(recognized_text)
        #await message.reply(f"üîä –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è:\n\n{pronunciation_feedback}")
        #logger.info(f"[handle_voice] ‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à—ë–Ω.")
#
        ## –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        #logger.info(f"[handle_voice] üìä –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑...")
        #analysis, text_analysis = await get_analysis(recognized_text)
        #if text_analysis:
        #    await message.reply(f"üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑:\n\n{text_analysis}")
        #    logger.info(f"[handle_voice] ‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.")

    except Exception as e:
        logger.exception(f"[handle_voice] üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        await message.reply("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.")

    finally:
        if ogg_path and os.path.exists(ogg_path):
            try:
                os.remove(ogg_path)
                logger.info(f"[handle_voice] üßπ –§–∞–π–ª —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {ogg_path}")
            except Exception as e:
                logger.error(f"[handle_voice] ‚ùó –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {ogg_path}: {e}")



#____________________________________________________________________________________
async def analyze_with_ollama(text: str) -> tuple[Optional[Dict], Optional[str]]:
    """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Ollama"""
    try:
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_words = len(text.split())
        unique_words = len(set(text.split()))
        sentences = re.split(r'[.!?]', text)  # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0

        # –°–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        conditional_sentences = len(re.findall(r"\b(if|unless|provided that)\b", text))  # –£—Å–ª–æ–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        inversions = len(re.findall(r"\b(had|should|were|do)\b.*\b(subject)\b", text))  # –ò–Ω–≤–µ—Ä—Å–∏–∏
        participles = len(re.findall(r"\b(\w+ing)\b", text))  # –ü—Ä–∏—á–∞—Å—Ç–∏—è
        gerunds = len(re.findall(r"\b(\w+ing)\b", text))  # –ì–µ—Ä—É–Ω–¥–∏–∏

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        prompt = f"""<s>[INST] –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –≤ –∞–Ω–∞–ª–∏–∑–µ —É—Ä–æ–≤–Ω–µ–π –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏:
    1. –£—Ä–æ–≤–µ–Ω—å CEFR (—Ç–æ–ª—å–∫–æ A, B –∏–ª–∏ C):
   - A: –ë–∞–∑–æ–≤—ã–π (–ø—Ä–æ—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å)
   - B: –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π (–º–æ–∂–µ—Ç –æ–±—â–∞—Ç—å—Å—è –Ω–∞ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ —Ç–µ–º—ã)
   - C: –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π (—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–ª–∞–¥–µ–Ω–∏–µ, —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)

    2. –£—Ä–æ–≤–µ–Ω—å —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –¥–ª—è –ê B –°(–ø–æ —à–∫–∞–ª–µ):
   –æ—Ç–≤–µ—Ç –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ: 1 –∏–ª–∏ 2
3. –ê–Ω–∞–ª–∏–∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å–ª–æ–≤
4. –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –û—à–∏–±–∫–∏ –≤ –≥–ª–∞–≥–æ–ª–∞—Ö, –∞—Ä—Ç–∏–∫–ª—è—Ö, –≤—Ä–µ–º–µ–Ω–∞—Ö
5. –ê–Ω–∞–ª–∏–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ
4. –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è:
   - –î–ª—è CEFR —É—á–∏—Ç—ã–≤–∞–π: 
     * –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
     * –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
     * –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –±–æ–≥–∞—Ç—Å—Ç–≤–æ
   - –ö–∞–∂–¥—É—é –æ—à–∏–±–∫—É –º–∞—Ä–∫–∏—Ä—É–π —Ç–æ—á–Ω—ã–º —Ç–∏–ø–æ–º
   
–§–æ—Ä–º–∞—Ç —Ç–æ–ª—å–∫–æ –∫–∞–∫ VALID JSON only:
{{
    "cefr_level": "B",
    "read_level": "–ø–æ–Ω—è—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç, —Å–ª–æ–∂–Ω–æ –ø–æ–Ω–∏–º–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç",
    "error_types": ["–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏e", "–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", "—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ"],
    "recommendations": ["...", "...", "..."],
    "key_mistakes": [
        {{"error": "...", "example": "...", "correction": "..."}}
    ],
    "statistics": {{
        "total_words": {total_words},
        "unique_words": {unique_words},
        "average_sentence_length": {avg_sentence_length},
        "conditional_sentences": {conditional_sentences},
        "inversions": {inversions},
        "participles": {participles},
        "gerunds": {gerunds}
    }}
}} 
Text: {text} [/INST]"""

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
        response = ollama.chat(model=OLLAMA_MODEL, messages=[{"role": "user", "content": prompt}])
        print(response)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á 'message'
        if 'message' not in response:
            logger.error(f"Ollama Response Error: No 'message' key found in response.")
            return None, None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ 'message' -> 'content'
        message_content = response['message'].get('content', '')
        if not message_content:
            logger.error(f"Ollama Response Error: 'content' field is empty.")
            return None, None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É JSON –≤ –æ–±—ä–µ–∫—Ç Python
        try:
            analysis = json.loads(message_content)
        except json.JSONDecodeError as e:
            logger.error(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –æ—Ç Ollama: {e}")
            logger.error(f"–ü–æ–ª—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {message_content}")
            return None, None

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        required_fields = ['cefr_level', 'read_level', 'key_mistakes', 'recommendations', 'statistics']
        for field in required_fields:
            if field not in analysis:
                logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
                return None, None

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram
        cefr_level = analysis.get('cefr_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        read_level = analysis.get('read_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        key_mistakes = analysis.get('key_mistakes', [])
        recommendations = analysis.get('recommendations', [])

        reply_message = f"üîç **–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è:** {cefr_level}\n\n"
        reply_message += f"–£—Ä–æ–≤–µ–Ω—å —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏: {read_level}\n\n"
        reply_message += "üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n"
        for rec in recommendations:
            reply_message += f" - {rec}\n"

        reply_message += "\n‚úçÔ∏è **–ö–ª—é—á–µ–≤—ã–µ –æ—à–∏–±–∫–∏:**\n"
        for mistake in key_mistakes:
            reply_message += f" - –û—à–∏–±–∫–∞: {mistake['error']}\n"
            reply_message += f"   –ü—Ä–∏–º–µ—Ä: {mistake['example']}\n"
            reply_message += f"   –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {mistake['correction']}\n"

        reply_message += "\nüìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞:**\n"
        stats = analysis.get('statistics', {})
        reply_message += f" - –í—Å–µ–≥–æ —Å–ª–æ–≤: {stats.get('total_words', 0)}\n"
        reply_message += f" - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞: {stats.get('unique_words', 0)}\n"
        reply_message += f" - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats.get('average_sentence_length', 0)} —Å–ª–æ–≤\n"
        reply_message += f" - –£—Å–ª–æ–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats.get('conditional_sentences', 0)}\n"
        reply_message += f" - –ò–Ω–≤–µ—Ä—Å–∏–∏: {stats.get('inversions', 0)}\n"
        reply_message += f" - –ü—Ä–∏—á–∞—Å—Ç–∏—è: {stats.get('participles', 0)}\n"
        reply_message += f" - –ì–µ—Ä—É–Ω–¥–∏–∏: {stats.get('gerunds', 0)}\n"

        return analysis, reply_message

    except Exception as e:
        logger.error(f"Ollama Error: {e}")
        return None, None

async def analyze_with_openai(text: str) -> Optional[str]:
    """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ OpenAI API"""
    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4o-mini",
            messages=[{
                "role": "system",
                "content": """–¢–æ—Ç –∂–µ –ø—Ä–æ–º–ø—Ç —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ..."""
            }, {
                "role": "user",
                "content": text
            }],
            temperature=0.7,
            max_tokens=1000
        )
        return response.choices[0].message['content']  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
    except Exception as e:
        logger.error(f"OpenAI Error: {e}")
        return None

async def get_analysis(text: str) -> tuple[Optional[Dict], Optional[str]]:
    """–†–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–µ—Ä–≤–∏—Å–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    if USE_OLLAMA:
        return await analyze_with_ollama(text)
    else:
        # Note: You'll need to modify analyze_with_openai similarly if you use it
        analysis = await analyze_with_openai(text)
        return None, analysis  # Assuming OpenAI returns just the text

@dp.message_handler(commands=['start'])
async def start(message: types.Message):
    """–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –∫–Ω–æ–ø–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    await message.answer("–ü—Ä–∏–≤–µ—Ç! –ù–∞–ø–∏—à–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —á—Ç–æ–±—ã —è –º–æ–≥ –æ—Ü–µ–Ω–∏—Ç—å —Ç–≤–æ–π —É—Ä–æ–≤–µ–Ω—å.", reply_markup=keyboard)

@dp.message_handler(lambda message: message.text != "–û—Ü–µ–Ω–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å")
async def collect_user_message(message: types.Message):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    user_id = message.from_user.id
    text = message.text

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if user_id not in user_messages:
        user_messages[user_id] = []  # –ï—Å–ª–∏ –µ—â–µ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫
    user_messages[user_id].append(text)

    await message.reply("üí¨ –í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

@dp.message_handler(text="–û—Ü–µ–Ω–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å")
async def evaluate_level(message: types.Message):
    """–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —è–∑—ã–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_id = message.from_user.id
    if not (messages := user_messages.get(user_id, [])):
        await message.reply("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ù–∞–ø–∏—à–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π.")
        return

    full_text = " ".join(messages)
    if not re.search(r'[a-zA-Z]', full_text):
        await message.reply("üî† –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º.")
        return

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º get_analysis –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞ analyze_with_ollama
    analysis, reply_text = await get_analysis(full_text)
    if not analysis or not reply_text:
        await message.reply("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞.")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    try:
        save_user_and_analysis(user_id, message.from_user.full_name, analysis)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î: {e}")
        await message.reply("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞.")

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    await message.reply(reply_text)


def save_user_and_analysis(user_id, name, analysis_dict):
    if not analysis_dict:
        logger.error("–ü—É—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    try:
        conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT)
        cur = conn.cursor()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        cur.execute("SELECT 1 FROM users WHERE user_id = %s", (user_id,))
        if not cur.fetchone():
            cur.execute(
                "INSERT INTO users (user_id, name) VALUES (%s, %s)",
                (user_id, name)
            )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤ –æ—à–∏–±–æ–∫
        spelling_errors = 0
        grammar_errors = 0
        syntax_errors = 0

        # 1. –ü–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫ –∏–∑ key_mistakes
        if isinstance(analysis_dict.get('key_mistakes'), list):
            for mistake in analysis_dict['key_mistakes']:
                error_type = mistake.get('type', '').lower()
                if error_type == 'spelling':
                    spelling_errors += 1
                elif error_type == 'grammar':
                    grammar_errors += 1
                elif error_type == 'syntax':
                    syntax_errors += 1

        # 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –∏–∑ error_types (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if isinstance(analysis_dict.get('error_types'), list):
            for error_type in analysis_dict['error_types']:
                error_type = error_type.lower()
                if 'spelling' in error_type or '–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ' in error_type:
                    spelling_errors += 1
                elif 'grammar' in error_type or '–≥—Ä–∞–º–º–∞—Ç' in error_type:
                    grammar_errors += 1
                elif 'syntax' in error_type or '—Å–∏–Ω—Ç–∞–∫—Å' in error_type:
                    syntax_errors += 1

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = analysis_dict.get('statistics', {})

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –±–∞–∑—É
        cur.execute("""
            INSERT INTO daily_analysis (
                user_id, analysis_date, cefr_level, read_level,
                recommendations, key_mistakes, total_words, unique_words,
                inversions, participles, gerunds, spelling_errors,
                grammar_errors, syntax_errors
            ) VALUES (
                %s, CURRENT_DATE, %s, %s, %s, %s::jsonb,
                %s, %s, %s, %s, %s, %s, %s, %s
            )
        """, (
            user_id,
            analysis_dict.get('cefr_level', 'Unknown'),
            analysis_dict.get('read_level', 'Unknown'),
            "\n".join(analysis_dict.get('recommendations', [])),
            json.dumps(analysis_dict.get('key_mistakes', [])),
            stats.get('total_words', 0),
            stats.get('unique_words', 0),
            stats.get('inversions', 0),
            stats.get('participles', 0),
            stats.get('gerunds', 0),
            spelling_errors,  # –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
            grammar_errors,
            syntax_errors
        ))

        conn.commit()
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –∞–Ω–∞–ª–∏–∑ –¥–ª—è user_id={user_id}. –û—à–∏–±–∫–∏: spelling={spelling_errors}, grammar={grammar_errors}, syntax={syntax_errors}")
        # –û—á–∏—â–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞
        if user_id in user_messages:
            del user_messages[user_id]
            logger.info(f"–°–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –æ—á–∏—â–µ–Ω—ã")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
        if conn: conn.rollback()
    finally:
        if cur: cur.close()
        if conn: conn.close()

if __name__ == '__main__':
    setup_database()
    logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    executor.start_polling(dp, skip_updates=True)
